---
jupyter:
  jupytext:
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.15.0
  kernelspec:
    display_name: Python 3 (ipykernel)
    language: python
    name: python3
---

```{python}
# %load_ext autoreload
# %autoreload 2
```

```{python}
import torch
import numpy as np
import time
import os
import json
import matplotlib.pyplot as plt    
```

```{python}
import phys_models.schwinger as sch
from utils import grab
#import normalizing_flow.flow as nf

from training.loss import REINFORCE_loss, rt_loss
from normalizing_flow.gauge_equivariant import make_schwinger_model
```

```{python}
# import utils.errors as er
```

```{python}
torch.__version__
```

```{python}
from scipy.stats import linregress
```

```{python}
torch.cuda.is_available()
```

```{python}
for dev in range(torch.cuda.device_count()):
    print(torch.cuda.get_device_properties(dev))
```

```{python}
torch_device="cuda:0"
float_dtype="float32"
```

```{python}
torch.cuda.get_device_properties(torch_device)
```

```{python}
# Theory
L = 8
lattice_shape = (L,L)
link_shape = (2,L,L)
beta = 2.0
kappa = 0.276
qed_action =  sch.QEDAction(beta, kappa)
```

```{python}
n_layers = 48
hidden_sizes = [64,64]
kernel_size = 3
n_knots=7
```

```{python}
model = make_schwinger_model(n_knots=n_knots, lattice_shape=lattice_shape, n_layers=n_layers,
                             hidden_sizes=hidden_sizes, kernel_size=kernel_size, dilations=[1,2,3],float_dtype="float32",
                             device=torch_device)

layers = model['layers']
prior = model['prior']
```

```{python}
prior.sample_n(10).shape
```

```{python}
loss_fn = REINFORCE_loss
```

```{python}
batch_size = 512
```

```{python}
z = prior.sample_n(batch_size=batch_size)
log_prob_z = prior.log_prob(z)
```

```{python}
starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)
loss_starter, loss_ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)
back_starter, back_ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)
rep = 40
timings=np.zeros((rep,3))
print("warming ... ")
for i in range(5):
    print(i)
    l, logq, logp = loss_fn(z, log_prob_z, model=model, action=qed_action, use_amp=False)
    l.backward()
print("measuring ... ")    
starter.record()
loss_starter.record()
l, logq, logp = loss_fn(z, log_prob_z, model=model, action=qed_action, use_amp=False)
loss_ender.record()
back_starter.record()
l.backward()
back_ender.record()
ender.record()

# WAIT FOR GPU SYNC
torch.cuda.synchronize()
for r in range(rep):
    if r%5==0:
        print(r)
    starter.record()
    loss_starter.record()
    l, logq, logp = loss_fn(z, log_prob_z, model=model, action=qed_action, use_amp=False)
    loss_ender.record()
    back_starter.record()
    l.backward()
    back_ender.record()
    ender.record()

    # WAIT FOR GPU SYNC
    torch.cuda.synchronize()
    curr_time = starter.elapsed_time(ender)
    timings[r,0] = starter.elapsed_time(ender)
    timings[r,1] = loss_starter.elapsed_time(loss_ender)
    timings[r,2] = back_starter.elapsed_time(back_ender)
```

```{python}
timings[1:].mean(0)/1000
```

```{python}
timings.std(0)/1000
```

```{python}
timings.mean(0)
```

```{python}
timings.std(0)
```

```{python}

```
