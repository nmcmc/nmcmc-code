---
jupyter:
  jupytext:
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.16.4
  kernelspec:
    display_name: Python 3 (ipykernel)
    language: python
    name: python3
---

```{python}
# %load_ext autoreload
# %autoreload 2
```

```{python}
import torch
import numpy as np
import time
import os
import json
import matplotlib.pyplot as plt    
```

```{python}
import neumc
import neumc.physics.schwinger as sch
from neumc.utils import grab

from neumc.training.loss import REINFORCE_loss, rt_loss
```

```{python}
torch.__version__
```

```{python}
from scipy.stats import linregress
```

```{python}
torch.cuda.is_available()
```

```{python}
for dev in range(torch.cuda.device_count()):
    print(torch.cuda.get_device_properties(dev))
```

```{python}
torch_device="cuda:0"
float_dtype="float32"
```

```{python}
torch.cuda.get_device_properties(torch_device)
```

```{python}
L = 8
lattice_shape = (L,L)
link_shape = (2,L,L)
beta = 2.0
kappa = 0.276
qed_action =  sch.QEDAction(beta, kappa)
```

```{python}
n_layers = 48
hidden_sizes = [64,64]
kernel_size = 3
n_knots=7

model_cfg = {'n_layers': 48,
                 'masking': '2x1',
                 'coupling': 'cs',
                 'nn': {
                     'hidden_channels': [64, 64],
                     'kernel_size': 3,
                     'dilation': [1, 2, 3],
                 },
                 'n_knots': 9,
                 'float_dtype': 'float32',
                 'lattice_shape': lattice_shape
                 }
```

```{python}
model = neumc.nf.u1_model_asm.assemble_model_from_dict(model_cfg, device=torch_device,verbose=1)
layers = model['layers']
prior = model['prior']
```

```{python}
prior.sample_n(10).shape
```

```{python}
loss_fn = REINFORCE_loss
```

```{python}
batch_size = 512
```

```{python}
z = prior.sample_n(batch_size=batch_size)
log_prob_z = prior.log_prob(z)
```

```{python}
starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)
loss_starter, loss_ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)
back_starter, back_ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)
rep = 40
timings=np.zeros((rep,3))
print("warming ... ")
for i in range(5):
    print(i)
    l, logq, logp = loss_fn(z, log_prob_z, model=model, action=qed_action, use_amp=False)
    l.backward()
print("measuring ... ")    
starter.record()
loss_starter.record()
l, logq, logp = loss_fn(z, log_prob_z, model=model, action=qed_action, use_amp=False)
loss_ender.record()
back_starter.record()
l.backward()
back_ender.record()
ender.record()

# WAIT FOR GPU SYNC
torch.cuda.synchronize()
for r in range(rep):
    if r%5==0:
        print(r)
    starter.record()
    loss_starter.record()
    l, logq, logp = loss_fn(z, log_prob_z, model=model, action=qed_action, use_amp=False)
    loss_ender.record()
    back_starter.record()
    l.backward()
    back_ender.record()
    ender.record()

    # WAIT FOR GPU SYNC
    torch.cuda.synchronize()
    curr_time = starter.elapsed_time(ender)
    timings[r,0] = starter.elapsed_time(ender)
    timings[r,1] = loss_starter.elapsed_time(loss_ender)
    timings[r,2] = back_starter.elapsed_time(back_ender)
```

```{python}
timings[1:].mean(0)/1000
```

```{python}
timings.std(0)/1000
```

```{python}
timings.mean(0)
```

```{python}
timings.std(0)
```

```{python}

```
