---
jupyter:
  jupytext:
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.15.0
  kernelspec:
    display_name: Python 3 (ipykernel)
    language: python
    name: python3
---

```{python}
# %load_ext autoreload
# %autoreload 2
```

# Memory

```{python}
import time
```

```{python}
import torch
import numpy as np
from scipy.stats import linregress
import matplotlib.pyplot as plt    
```

```{python}
torch.__version__
```

```{python}
import utils.profile
import training.train
import training.loss
```

```{python}
from normalizing_flow.gauge_equivariant import make_schwinger_model
import phys_models.schwinger as sch
from utils import grab
```

```{python}
torch.cuda.is_available()
```

```{python}
for dev in range(torch.cuda.device_count()):
    print(torch.cuda.get_device_properties(dev))
```

```{python}
torch_device = "cuda:0"
float_dtype ="float32"
```

```{python}
L = 16
lattice_shape = (L,L)
link_shape = (2,L,L)
beta = 2.0
kappa = 0.276
qed_action =  sch.QEDAction(beta, kappa)
```

```{python}
n_layers = 48
hidden_sizes = [64,64]
kernel_size = 3
n_knots = 9
```

```{python}
model = make_schwinger_model(n_knots=n_knots, lattice_shape=lattice_shape, n_layers=n_layers,
                             hidden_sizes=hidden_sizes, kernel_size=kernel_size, dilations=[1,2,3],float_dtype="float32",
                             device=torch_device)
layers = model['layers']
prior = model['prior']
```

```{python}
loss_fn = training.loss.rt_loss
```

```{python}
batch_size = 128
```

```{python}
z = prior.sample_n(batch_size=batch_size)
log_prob_z = prior.log_prob(z)
```

```{python}
torch.cuda.reset_peak_memory_stats()
```

```{python}
with torch.autograd.graph.saved_tensors_hooks(pack_hook:=utils.profile.MemoryPackHook(), utils.profile.unpack_hook):
    l, logq, logp = loss_fn(z, log_prob_z, model=model, action=qed_action, use_amp=False)
```

```{python}
print(f"{pack_hook.mem(b=30):.2f}GB")
```

```{python}
print(f"{pack_hook.mem_u(b=30):.2f}GB")
```

```{python}
len(pack_hook.ptrs)
```

```{python}
len(set(pack_hook.ptrs))
```

```{python}
print(f"{torch.cuda.max_memory_allocated()/2**30:.2f}GB")
```

```{python}
l.backward()
```

```{python}
print(f"{torch.cuda.max_memory_allocated()/2**30:.2f}GB")
```

```{python}

```

# 
