---
jupyter:
  jupytext:
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.16.4
  kernelspec:
    display_name: Python 3 (ipykernel)
    language: python
    name: python3
---

```{python}
# %load_ext autoreload
# %autoreload 2
```

# Memory

```{python}
import time
```

```{python}
import torch
import numpy as np
from scipy.stats import linregress
import matplotlib.pyplot as plt    
```

```{python}
torch.__version__
```

```{python}
import neumc
```

```{python}
from neumc.nf.u1_model_asm import assemble_model_from_dict
import neumc.physics.schwinger as sch
from neumc.utils import grab
```

```{python}
torch.cuda.is_available()
```

```{python}
for dev in range(torch.cuda.device_count()):
    print(torch.cuda.get_device_properties(dev))
```

```{python}
torch_device = "cuda:0"
float_dtype ="float32"
```

```{python}
L = 16
lattice_shape = (L,L)
link_shape = (2,L,L)
beta = 2.0
kappa = 0.276
qed_action =  sch.QEDAction(beta, kappa)
```

```{python}
n_layers = 48
hidden_sizes = [64,64]
kernel_size = 3
n_knots = 9

model_cfg = {'n_layers': 48,
                 'masking': '2x1',
                 'coupling': 'cs',
                 'nn': {
                     'hidden_channels': [64, 64],
                     'kernel_size': 3,
                     'dilation': [1, 2, 3],
                 },
                 'n_knots': 9,
                 'float_dtype': 'float32',
                 'lattice_shape': lattice_shape
                 }
```

```{python}
model = assemble_model_from_dict(model_cfg, device=torch_device, verbose=1)

layers = model['layers']
prior = model['prior']
```

```{python}
loss_fn = neumc.training.loss.rt_loss
```

```{python}
batch_size = 128
```

```{python}
z = prior.sample_n(batch_size=batch_size)
log_prob_z = prior.log_prob(z)
```

```{python}
torch.cuda.reset_peak_memory_stats()
```

```{python}
with torch.autograd.graph.saved_tensors_hooks(pack_hook:=neumc.utils.profile.MemoryPackHook(), neumc.utils.profile.unpack_hook):
    l, logq, logp = loss_fn(z, log_prob_z, model=model, action=qed_action, use_amp=False)
```

```{python}
print(f"{pack_hook.mem(b=30):.2f}GB")
```

```{python}
print(f"{pack_hook.mem_u(b=30):.2f}GB")
```

```{python}
len(pack_hook.ptrs)
```

```{python}
len(set(pack_hook.ptrs))
```

```{python}
print(f"{torch.cuda.max_memory_allocated()/2**30:.2f}GB")
```

```{python}
l.backward()
```

```{python}
print(f"{torch.cuda.max_memory_allocated()/2**30:.2f}GB")
```
