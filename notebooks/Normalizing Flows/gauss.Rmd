---
jupyter:
  jupytext:
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.15.0
  kernelspec:
    display_name: Python 3 (ipykernel)
    language: python
    name: python3
---

```{python}
import torch 
import  numpy as np
```

```{python}
import matplotlib.pyplot as plt
```




# Normalizing flows


$\newcommand{\bz}{\mathbf{z}}\newcommand{\btheta}{\boldsymbol{\theta}}$
$$\mathbb{R}^{D}\ni \bz\longrightarrow (q_{pr}(\bz),\boldsymbol{\varphi}(\bz|\btheta))\in (\mathbb{R},\mathbb{R}^{D}),
$$


\begin{equation}
q(\mathbf{\phi}|\btheta)= q_z(\bz|\btheta) \equiv  q_{pr}(\bz)\left|J(\bz|\btheta)^{-1}\right|,\quad \mathbf{\phi}=\mathbf{\varphi}(\bz|\btheta),
\end{equation}


\begin{equation}
    J(\bz|\btheta)=\det\mathbf{J}
\end{equation}


$$
\mathbf{J}=\left(\frac{\partial\boldsymbol{\varphi}(\bz|\btheta)}{\partial\bz}\right)
=\begin{pmatrix}
\frac{\partial{\varphi_1}(\bz|\btheta)}{\partial z_1} & \frac{\partial{\varphi_1}(\bz|\btheta)}{\partial z_2} & \cdots\\
\frac{\partial{\varphi_2}(\bz|\btheta)}{\partial z_1} & \frac{\partial{\varphi_2}(\bz|\btheta)}{\partial z_2} & \cdots\\
\vdots & \vdots
\end{pmatrix}
$$


### Exponential distribution


Probably the simplest example of normalizing flow is the exponential distribution that can be obtained from uniform distribution by the transformation 


$$\phi=-\frac{1}{\lambda}\log(1-z)$$


where $z$ is uniformly distributed on interval $[0,1)$. Inverting this flow we obtain 


$$z(\phi) = 1-e^{-\lambda\phi}.$$


Calculating the Jacobian which is this case is a single number we indeed obtain the exponential distribution


$$\left(\frac{\partial \phi(z)}{\partial z}\right)^{-1}=\frac{\partial z(\phi)}{\partial\phi} = \lambda e^{-\lambda\phi}.$$


$$\phi(z|\theta)=-\frac{1}{\theta}\log(1-z)$$


$$D_{KL}(q|p)=\int_0^\infty\!\text{d}\phi\, q(\phi|\theta)\left(\log q(\phi|\theta)-\log p(\phi)\right)
=\theta\int_0^\infty\!\text{d}\phi\, e^{-\theta\phi}\left(\log\theta-\log\lambda -\theta\phi +\lambda\phi\right)=
\frac{\lambda}{\theta}-\log\frac{\lambda}{\theta} -1
$$

```{python}
def dkl_exp(lamda,theta):
    return -np.log(lamda/theta)+lamda/theta-1
```

```{python}
def dkl_exp_grad(lamda, theta):
    return (lamda-theta)/theta**2
```

```{python}
lamda = 2.0
thetas=np.linspace(.5,4,500)
plt.plot(thetas, dkl_exp(lamda, thetas));
```

We can change the variables in the above integral to $z$, by substituting $\phi=\varphi(z|\theta)$


$$D_{KL}(q|p)=\int_0^1\!\text{d}z\, \frac{d \varphi(z|\theta)}{d z} q(\varphi(z|\theta)|\theta)
\left(\log q(\varphi(z|\theta)|\theta)-\log p(\varphi(z|\theta))\right)
=\int_0^1\!\text{d}z\, q_{pr}(z)
\left(\log q(\varphi(z|\theta)|\theta)-\log p(\varphi(z|\theta))\right)
$$


This is so called _reparameterisation trick_. It is left for the reader to verify that it does indeed produce same result.


### Training 


$$\frac{d D_{KL}(q|p)}{d \theta}=\frac{\lambda-\theta}{\theta^2}$$

```{python}
lamda = 2.0
thetas=np.linspace(.5,4,500)
plt.plot(thetas, dkl_exp_grad(lamda, thetas));
plt.axhline(0, color='black');
```

$\newcommand{\avg}[1]{\left\langle #1\right\rangle}$
$$D_{KL}(q|p)=\avg{\log q(\phi)-\log p(\phi)}_q(\phi)\approx\frac{1}{n_s}\sum_{i=1}^{n_s}(\log q(\phi_i) -\log p(\phi_i)),\quad \phi_i\sim q(\phi_i)$$

```{python}
def flow(z,th):
    return -1/th * torch.log(1-z)
```

```{python}
def q(phi,th):
    return th*torch.exp(-th*phi)

def p(phi):
    return q(phi,lamda)
```

```{python}
theta = torch.FloatTensor([1.0])
theta.requires_grad_(True)
```

```{python}
n_samples=2**16
with torch.no_grad():
    z = torch.rand(n_samples)
    phi = flow(z,theta)
    log_q = torch.log(q(phi,theta))
```

```{python}
phis=torch.linspace(0,3,500)
plt.hist(phi,bins=100, density=True);
plt.plot(phis, q(phis, theta.detach()))
```

```{python}
batch_size= 2**16
```

```{python}
optimizer = torch.optim.Adam((theta,),lr=0.1)
```

```{python}
history = {'loss':[], 'theta':[]}
```

```{python}
from utils.live_plot import init_live_plot, update_plots
```

```{python}
n_epochs=100
live_plot = init_live_plot(1,n_epochs, plot_ess=False)
for epoch in range(n_epochs):
    optimizer.zero_grad()
    z = torch.rand(batch_size)
    phi = flow(z,theta)
    log_q = torch.log(q(phi,theta))
    log_p = torch.log(p(phi))
    dkl = torch.mean(log_q-log_p)
    dkl.backward()
    history['loss'].append(dkl.detach().item())
    history['theta'].append(theta.detach().item())
                    
    optimizer.step()    
    update_plots(history, **live_plot)
```

```{python}
fig, ax = plt.subplots(figsize=(8,4))
ax.axhline(lamda,c='black')
ax.set_ylabel("$\\theta$",rotation='horizontal')
ax.set_xlabel("epoch")
ax.plot(np.arange(len(history['theta']))+1, history['theta']);
ax.margins(0.01);
```

## Normal distribution


$$x_0=\cos(2\pi z_0)\sqrt{-2\log(1-z_1)}\quad x_1=\sin(2\pi z_0)\sqrt{-2\log(1-z_1)}$$


$$x_0^2+x_1^2 =-2\log(1-z_1)$$


$$z_1=1-e^{-\frac{1}{2}(x_0^2+x_1^2)} $$


$$\frac{x_1}{x_0}=\tan 2\pi z_0$$


$$z_0=\frac{1}{2\pi}\tan^{-1}\frac{x_1}{x_0}$$ 


$$
\mathbf{J}^{-1}=\begin{pmatrix}
\frac{\partial z_0}{\partial x_0} & \frac{\partial z_0}{\partial x_1}\\
\frac{\partial z_1}{\partial x_0} & \frac{\partial z_1}{\partial x_1}
\end{pmatrix}=
\begin{pmatrix}
-\frac{1}{2\pi}\frac{x_1}{x_0^2+x_1^2} & \frac{1}{2\pi}\frac{x_0}{x_0^2+x_1^2}\\
x_0 e^{-\frac{1}{2}(x_0^2+x_1^2)} & x_1 e^{-\frac{1}{2}(x_0^2+x_1^2)}
\end{pmatrix}
$$


$$J(x(z))^{-1} = \det\mathbf{J}^{-1}=
-\frac{1}{2\pi}\frac{x_1^2}{x_0^2+x_1^2} e^{-\frac{1}{2}(x_0^2+x_1^2)}-\frac{1}{2\pi}\frac{x_0^2}{x_0^2+x_1^2} e^{-\frac{1}{2}(x_0^2+x_1^2)}
= -\frac{1}{2\pi} e^{-\frac{1}{2}(x_0^2+x_1^2)}
$$


##  Affine coupling layers


$$ \begin{split}
x'_0 & =   e^{s(x_1)}x_0+t(x_1)\\
x'_1 & =  x_1
\end{split}
$$


$$ \begin{split}
x'_0 & =   e^{s(x_1)}x_0+t(x_1)\\
x'_1 & =  x_1
\end{split}
$$


$$
\begin{split}
x_0 & = e^{-s(x'_1)}(x'_0-t(x'_1))\\
x_1 & = x'_1
\end{split}
$$


$$\mathbf{J}=
\begin{pmatrix}
e^{s(x_1)} & e^{s(x_1)}\frac{\partial s(x_1)}{\partial x_1}\\
0 & 1
\end{pmatrix}
$$


$$\det \mathbf{J}^{-1} = e^{-s(x_1)}$$


$$ \begin{split}
x''_0 & =  x'_0\\
x''_1 & =   e^{s(x'_0)}x'_1+t(x'_0)
\end{split}
$$

```{python}
from utils.errors import is_model_not_finite, are_grad_not_finite, is_not_finite
```

```{python}
class AffineCoupling(torch.nn.Module):
    def __init__(self,i, net, device='cpu'):
        super().__init__()
        self.mask = torch.zeros(2).to(device=device)
        self.mask[i] = 1.0
        self.net = net

    def forward(self,x):
        frozen = x*(self.mask-1)
        net_out = self.net(frozen)
        s,t = net_out[:,0:2],net_out[:,2:4]
        return (torch.exp(s)*x+t)*self.mask + x*(self.mask-1), torch.sum(self.mask*s,1)
    
```

```{python}
def make_affine_layers(n_layers, device='cpu'):
    layers = []
    for i in range(n_layers):
        parity = i % 2
        net=torch.nn.Sequential(
            torch.nn.Linear(2,32),torch.nn.LeakyReLU(),
            torch.nn.Linear(32,32),torch.nn.LeakyReLU(),
            torch.nn.Linear(32,4), torch.nn.Tanh()
        ).to(device=device)
        layers.append(AffineCoupling(parity, net, device=device) )
    return torch.nn.ModuleList(layers)                  
        
```

```{python}
def apply_flow(layers, za,logza):
    lz = torch.zeros_like(logza, device=logza.device)
    for l in layers:
        z , logJ = l.forward(za)
        lz -= logJ
        za = z
    return z, lz+logza    
        
```

```{python}
layers = make_affine_layers(32)
```

```{python}
z,log_propb_z = torch.rand(4,2), torch.zeros(4)
```

```{python}
apply_flow(layers, z, log_propb_z)
```

```{python}
def num_jacobian(layers, x, epsilon = 1e-3, device='cpu'):
    e0 = torch.zeros_like(x, device=device)
    e0[:,0] = epsilon
    e1 = torch.zeros_like(x, device=device)
    e1[:,1] = epsilon
    with torch.no_grad():
        out, _ = apply_flow(layers, x, torch.zeros(len(x)))
        out0, _ = apply_flow(layers, x+e0, torch.zeros(len(x)))
        out1, _ = apply_flow(layers, x+e1, torch.zeros(len(x)))
    j =   torch.zeros(len(x),2,2, device=device)
    j[:,0,:]=(out0-out)/epsilon
    j[:,1,:]=(out1-out)/epsilon
    return torch.logdet(j)
```

```{python}
num_jacobian(layers,z)
```

### Training

```{python}
prior = torch.distributions.uniform.Uniform(0,1)
```

```{python}
def action(x):
    return 0.5*torch.sum(x**2,1)+np.log(2*torch.pi)
```

```{python}
batch_size = 2*16
```

```{python}
layers = make_affine_layers(32, device='cuda').to(device='cuda')
```

```{python}
optim = torch.optim.Adam(layers.parameters(),lr=0.0001)
```

```{python}
# %%time
layers.train()
for epoch in range(8000):
    if is_model_not_finite(layers):
        print(f"model is not finite")
        break
    z = prior.rsample( (batch_size,2)).to(device='cuda')
    x,log_q = apply_flow(layers, z,prior.log_prob(z).sum(1).to(device='cuda') ) 
    
                     
    log_p = -action(x)
    loss = torch.mean(log_q-log_p)
    if epoch % 100 == 0:
        print(epoch, loss.detach())
    loss.backward()
    
    torch.nn.utils.clip_grad_value_(layers.parameters(), 0.1)    
    torch.nn.utils.clip_grad_norm_(layers.parameters(),1.0)
    
    optim.step()
print(loss.detach())    
layers.eval()
```

```{python}
with torch.no_grad():
    z = torch.rand(2**16,2, device='cuda')
    x, log_q = apply_flow(layers,z, prior.log_prob(z).sum(1).to(device='cuda') )
    log_p = -action(x)
    print(f"DKL = {(log_q-log_p).mean():.3f}")
```

```{python}
from scipy.stats import norm
```

```{python}
xs = np.linspace(-3,3,500)
plt.hist(x.cpu().ravel(), bins=100, density=True);
plt.plot(xs, norm.pdf(xs)); 
```

```{python}

```
