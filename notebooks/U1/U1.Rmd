---
jupyter:
  jupytext:
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.15.1
  kernelspec:
    display_name: Python 3 (ipykernel)
    language: python
    name: python3
---

```{python}
# Helps with the debuging. The  functions are reloaded each time they are used. 
# %load_ext autoreload
# %autoreload 2
```

```{python}
import matplotlib.pyplot as plt    
```

```{python}
from utils.lattice_plots import plot_lattice, plaquette
```

# U(1) Pure gauge model. 


$U(1)$ group is a group of all complex numbers with absolute value 1 and the elements of this group are completely detrmined by a single phase $\theta$: $U(x)=e^{i\theta(x)}$. In the lattice formulation the elements of this group live on the links of the lattice 

```{python}
fig, ax = plt.subplots()
plot_lattice(8, ax=ax);
```

$$U_\mu(x)=e^{i\theta_\mu(x)}$$


$U_\mu(x)$ is a link starting from $x$ in direction $\mu=0,1$ and $\hat\mu$ is the displacement vector of one lattice site in the direction $\mu$.  Given the gauge transform $g(x)$  where each $g(x)=e^{i\phi(x)}$ is an element of $U(1)$ living on the site $x$ the lattice the action of this transformation on links is defined by


$$U_\mu(x)\longrightarrow g(x)U(x)_\mu g^{-1}(x+\hat\mu),\qquad \theta_\mu(x)\longrightarrow \phi(x)+\theta_\mu(x)-\phi(x+\hat\mu)$$


We define a plaquette as a product of links around a $1\times1$ loop (links traversed in opposite direction are conjugated)

```{python}
ax = plot_lattice(4, l=0.45, cr=0.05)
ax.set_aspect(1.0);ax.set_axis_off()
for x in range(4):
    for y in range(4):
        plaquette(x,y, color='blue', l=0.275, r=0.05, d=0.125, ax = ax)
```

\begin{equation}
    P(x)=U_1(x) U_0(x+\hat{1}) U_1^\dagger(x+\hat{0}) U_0^\dagger (x) = e^{i\left(\theta_1(x) + \theta_0(x+\hat{1}) -\theta_1(x+\hat{0}) -\theta_0^(x)\right)}
\end{equation}


It's easy to check that plaquettes (as any other loops) are gauge invariant. The Wilson action is  finally defined in term of the plaquettes


$\newcommand{\re}{\operatorname{Re}}
\newcommand{\im}{\operatorname{Im}}$
\begin{equation}
    S(U) = -\beta\sum_{x}\re P(x)=-\beta\sum_{x}\cos\left(\theta_1(x) + \theta_0(x+\hat{1}) -\theta_1(x+\hat{0}) -\theta_0^(x)\right)
\end{equation}

```{python}
import numpy as np
import torch
print(f'Running with torch version {torch.__version__}')
```

```{python}
from scipy.stats import linregress
```

```{python}
from normalizing_flow import rational_splines_u1 as rs    
from utils import grab
import normalizing_flow.u1_equivariant as equiv
import normalizing_flow.non_compact_projection  as ncp
import normalizing_flow.circular_splines_equivariant_couplings as cs
from normalizing_flow.gauge_equivariant import make_u1_nc_model
```

```{python}
torch.cuda.is_available()
```

```{python}
for dev in range(torch.cuda.device_count()):
    print(torch.cuda.get_device_properties(dev))
```

```{python}
torch_device="cuda:0"
float_dtype="float32"
```

```{python}
torch.cuda.get_device_properties(torch_device)
```

```{python}
batch_size = 1024
```

```{python}
import phys_models.U1 as u1
import normalizing_flow.flow as nf
```

## Tests


Some "sanity tests".

```{python}
L = 8
lattice_shape = (L,L)
link_shape = (2,L,L)
```

Some two arbitrary random configurations

```{python}
u1_ex1 = 2*np.pi*np.random.random(size=link_shape).astype(float_dtype)
u1_ex2 = 2*np.pi*np.random.random(size=link_shape).astype(float_dtype)
cfgs = torch.from_numpy(np.stack((u1_ex1, u1_ex2), axis=0)).to(torch_device)
```

```{python}
beta = 1.0
u1_action = u1.U1GaugeAction(beta)
```

Action should be invariant under gauge transformation

```{python}
cfgs_transformed = u1.random_gauge_transform(cfgs, device=torch_device)
print(grab(u1_action(cfgs)), 'vs', grab(u1_action(cfgs_transformed)))
assert np.allclose(grab(u1_action(cfgs)), grab(u1_action(cfgs_transformed)),atol=1e-5), \
    'gauge transform should be a symmetry of the action'
```

```{python}
assert np.allclose(grab(u1.compute_u1_2x1_loops(cfgs)),grab(u1.compute_u1_2x1_loops(cfgs_transformed)),atol=1e-5)
```

```{python}
with np.printoptions(suppress=True):
    print(f'cfg topological charges = {grab(u1.topo_charge(cfgs))}')
Q = grab(u1.topo_charge(cfgs))
assert np.allclose(Q, np.around(Q), atol=1e-5), 'topological charge must be an integer'
```

# Free energy


The partition function of this model is given by (R. Janik private comminication)


$$Z(\beta)=I_0(\beta)^{L^2} + 2\sum_{n=1}^\infty I_n(\beta)^{L^2}$$


where $I_n(\beta)$ are incomplete Bessel functions. The logarithm of $Z$ can be written as 


$$\log Z(\beta)=(2\pi)^{-2L^2}\left(
L^2\log I_0(\beta) + \log\left(
    1 + 2\sum_{n=1}^\infty \left(\frac{I_n(\beta)}{I_0(\beta)}\right)^{L^2}
    \right)\right)
$$


The log of this function is  implemented (approximately) in module `phys_models.U1`.  

```{python}
F=-u1.logZ(L,beta)
print(f'Exact F = {F:.4f}')
```

## Prior distrubution 


Prior distribution is uniform on interval $[0,2\pi)$, so the probability density for each configurations is $(2\pi)^{-2*L^2}$.

```{python}
# Prior
prior = u1.MultivariateUniform(torch.zeros(link_shape), 2*np.pi*torch.ones(link_shape), device=torch_device).to(torch_device)
```

```{python}
z = prior.sample_n(16)
print(f'z.shape = {z.shape}')
assert np.allclose(grab(prior.log_prob(z)),-2*L*L*np.log(2*np.pi))
```

### Plaquettes transformation


The plaquette transform must be a diffeomorphism $f(\theta): [0,2\pi]\rightarrow [0,2\pi]$. For it is   sufficient that 
$$
\begin{split}
&f(0)=0,\; f(2\pi)=2\pi\\
&\frac{\partial f(\theta)}{\theta}>0\\
&\left.\frac{\partial f(\theta)}{\theta}\right|_{\theta=0} = \left.\frac{\partial f(\theta)}{\theta}\right|_{\theta=2\pi}
\end{split}
$$


## Non-compact projection (NCP) coupling


Non-compact projection is a transformation of the form


$$
f(\theta,\alpha, \beta)= 2 \tan^{-1}\left(\alpha\tan\left(\frac{\theta}{2}\right)+\beta\right),\quad \alpha>0
$$

```{python}
thetas=torch.linspace(0,2*torch.pi-1e-7,500)
for s in [0,0.5,1,5]:
    plt.plot(thetas, ncp.tan_transform(thetas,torch.FloatTensor([s])), label=f"{s:.2f}");
plt.xlabel('$\\theta$')    
plt.legend(title='s');    
```

In our implementation we set $\beta=0$ and $\alpha=\exp(s)$ where $s$ is an arbitrary real number. The final transformation is a mean of $n_m$ such transforms plus an offset


$$g(\theta,\vec{s},t)=\mod\left(\frac{1}{n_m}\sum_{i=1}^{n_m} f(\theta,\exp(s_i),0)+t, 2\pi\right)$$

```{python}
s = torch.FloatTensor([10,0.1,3,0.1,1,0])
plt.plot(
    thetas,  ncp.mixture_tan_transform(thetas.unsqueeze(1),s.unsqueeze(0)).squeeze()
        );
plt.xlabel('$\\theta$')  ;
```

### Plaquettes

```{python}
z = prior.sample_n(10)
```

```{python}
plaq = u1.compute_u1_plaq(z,0,1)
```

```{python}
plaq.mean()
```

```{python}
plaq.shape
```

```{python}
plaq_trans = ncp.tan_transform(plaq,s=torch.zeros_like(plaq))
```

```{python}
torch.abs(plaq-plaq_trans).mean()
```

```{python}
plaq_trans = ncp.tan_transform(plaq,10*torch.ones_like(plaq))
```

```{python}
torch.abs(plaq-plaq_trans).mean()
```

### Training

```{python}
# Model parameters

#Common model parameters
n_layers = 16
n_s_nets = 2
hidden_sizes = [8,8]
kernel_size = 3

n_mixture_comps = 6 
```

```{python}
model = make_u1_nc_model(type='sch', n_mixture_comps=n_mixture_comps, lattice_shape=lattice_shape, n_layers=n_layers,
                             hidden_sizes=hidden_sizes, kernel_size=kernel_size, float_dtype='float32', dilation=1, device=torch_device, verbose=1)
pior = model['prior']
layers = model['layers']

# Training
base_lr = .001
optimizer = torch.optim.Adam(model['layers'].parameters(), lr=base_lr)
#optimizer = to
```

```{python}
N_era = 6
N_epoch = 50
print_freq = N_epoch # epochs
plot_freq = 5 # epochs

history = {
    'dkl' : [],
    'std_dkl': [],
    'loss' : [],
    'ess' : []
}
```

```{python}
from training.train import train_step
from training.loss import REINFORCE_loss, rt_loss
from utils.metrics import add_metrics

```

```{python}
from utils.live_plot import init_live_plot, update_plots
import utils.metrics as um
import time
```

```{python}
# %%time
[plt.close(plt.figure(fignum)) for fignum in plt.get_fignums()] # close all existing figures
live_plot = init_live_plot(N_era, N_epoch)
start_time = time.time()

for era in range(N_era):
    for epoch in range(N_epoch):
        m=train_step(use_amp=True, model=model, action=u1_action, loss_fn=rt_loss, batch_size=batch_size, optimizer=optimizer)
        um.add_metrics(history, m)
        
        if (epoch+1) % print_freq == 0:
            avg = um.average_metrics(history, N_epoch, history.keys())
            ellapsed_time = time.time()-start_time
            print(f"Era {era:3d} epoch {epoch:4d} elapsed time {ellapsed_time:.1f}")
            um.print_dict(avg)


        if (epoch+1) % plot_freq == 0:
            update_plots(history, **live_plot)

```

```{python}
u,lq = nf.sample(batch_size=512,n_samples=2**12, prior=prior, layers=layers)
```

```{python}
lp = -nf.calc_action(u,batch_size=1024, action=u1_action, device=torch_device)
```

```{python}
u.shape
```

```{python}
lw=lp-lq
```

```{python}
F_eff = nf.calc_dkl(lp,lq)
print(F_eff)
```

```{python}
(F-F_eff)/F
```

```{python}
F_w =-(torch.logsumexp(lw,0)-np.log(len(lw)))
```

```{python}
(F_w-F)/F
```

```{python}
nf.compute_ess(lp,lq)
```

```{python}
r = linregress(lp, lq)
```

```{python}
fig, ax = plt.subplots(figsize=(6,6))
ax.set_aspect(1)
lps = np.linspace(lp.min(), lp.max(),100)
ax.plot(lps, r.slope*lps+r.intercept,'-r')
ax.scatter(lp, lq,s=2,alpha=0.25);
plus = '+' if r.intercept>0 else ''
ax.text(0.1,0.8,f"{r.slope:.2f}x{plus}{r.intercept:.2f}",transform=ax.transAxes)
```

```{python}
from monte_carlo.nmcmc import metropolize
```

```{python}
u_p, s_p, s_q, accepted = metropolize(u, lq, lp)
```

```{python}
print("Accept rate:", grab(accepted).mean())
```

```{python}
Q = grab(u1.topo_charge(u_p))
plt.figure(figsize=(5,3.5), dpi=125)
plt.plot(Q)
plt.xlabel(r'$t_{MC}$')
plt.ylabel(r'topological charge $Q$')
plt.show()
```

```{python}
from utils.stats_utils import bootstrap
```

```{python}
X_mean, X_err = bootstrap(Q**2, n_samples=100, binsize=16)
print(f'Topological susceptibility = {X_mean:.2f} +/- {X_err:.2f}')
print(f'... vs HMC estimate = 1.23 +/- 0.02')
```


[59]:
￼
## Rational splines coupling

```{python}
n_knots = 9 
```

```{python}
layers = cs.make_u1_equiv_layers_rs_with_2x1_loops(n_knots=n_knots, lattice_shape=lattice_shape, n_layers=n_layers,
                             hidden_sizes=hidden_sizes, kernel_size=kernel_size, float_dtype="float32",
                             device=torch_device)

u1.set_weights(layers)
model = {'layers': layers, 'prior': prior}
model['layers'].to(torch_device)
# Training
base_lr = .001
optimizer = torch.optim.Adam(model['layers'].parameters(), lr=base_lr)
```

```{python}
layers, prior = model['layers'], model['prior']
torch_x, torch_logq = nf.apply_flow_to_prior(prior, layers, batch_size=1024)
```

```{python}
torch_x.shape
```

```{python}
torch_logq
```

```{python}
N_era = 6
N_epoch = 50
print_freq = N_epoch # epochs
plot_freq = 1 # epochs

history_rs = {
    'dkl' : [],
    'std_dkl': [],
    'loss' : [],
    'ess' : []
}
```

```{python}
# %%time
[plt.close(plt.figure(fignum)) for fignum in plt.get_fignums()] # close all existing figures
live_plot = init_live_plot(N_era, N_epoch)
start_time = time.time()

for era in range(N_era):
    for epoch in range(N_epoch):
        m=train_step(use_amp=True, model=model, action=u1_action, loss_fn=REINFORCE_loss, batch_size=batch_size, optimizer=optimizer)
        um.add_metrics(history_rs, m)
        
        if (epoch+1) % print_freq == 0:
            avg = um.average_metrics(history_rs, N_epoch, history.keys())
            ellapsed_time = time.time()-start_time
            print(f"Era {era:3d} epoch {epoch:4d} ellapsed time {ellapsed_time:.1f}")
            um.print_dict(avg)


        if (epoch+1) % plot_freq == 0:
            update_plots(history_rs, **live_plot)


```

```{python}
u,lq = nf.sample(batch_size=1024,n_samples=2**12, prior=prior, layers=layers)
```

```{python}
lp = -nf.calc_action(u,batch_size=1024, action=u1_action, device=torch_device)
```

```{python}
lw=lp-lq
```

```{python}
F_eff = nf.calc_dkl(lp,lq)
print(F_eff)
```

```{python}
(F-F_eff)/F
```

```{python}
F_w =-(torch.logsumexp(lw,0)-np.log(len(lw)))
```

```{python}
(F_w-F)/F
```

```{python}
nf.compute_ess(lp,lq)
```

```{python}
r = linregress(lp, lq)
```

```{python}
fig, ax = plt.subplots(figsize=(6,6))
ax.set_aspect(1)
lps = np.linspace(lp.min(), lp.max(),100)
ax.plot(lps, r.slope*lps+r.intercept,'-r')
ax.scatter(lp, lq,s=2,alpha=0.25);
plus = '+' if r.intercept>0 else ''
ax.text(0.1,0.8,f"{r.slope:.2f}x{plus}{r.intercept:.2f}",transform=ax.transAxes)
```

```{python}
from monte_carlo.nmcmc import metropolize
```

```{python}
u_p, s_p, s_q, accepted = metropolize(u, lq, lp)
```

```{python}
print("Accept rate:", grab(accepted).mean())
```

```{python}
Q = grab(u1.topo_charge(u_p))
plt.figure(figsize=(5,3.5), dpi=125)
plt.plot(Q)
plt.xlabel(r'$t_{MC}$')
plt.ylabel(r'topological charge $Q$')
plt.show()
```

```{python}
X_mean, X_err = bootstrap(Q**2, n_samples=100, binsize=16)
print(f'Topological susceptibility = {X_mean:.2f} +/- {X_err:.2f}')
```
